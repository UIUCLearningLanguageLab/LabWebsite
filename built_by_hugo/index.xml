<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>L&amp;L Lab</title>
    <link>/</link>
    <description>Recent content on L&amp;L Lab</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About our Lab</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>How do we learn and use language? Computational Modeling The goal of our research is to better understand the mechanisms underlying the mastery of linguistic knowledge. Recent advances in our ability to study large, naturalistic datasets, combined with advanced computational modeling techniques, have allowed us to ask questions that were not possible just years before. One of the major insights from our work is that “Big Data” research that investigates the structure of experience – such as investigations of giant corpora of naturalistic speech – will force us to radically re-evaluate certain theories of learning and representation.</description>
    </item>
    
    <item>
      <title>Join</title>
      <link>/join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/join/</guid>
      <description>Graduate Students Dr. Montag and Dr. Willits are not recruiting graduate students for Fall 2021.
Post-Docs Dr. Willits may be recruiting a post-doc to begin Fall 2021. The research focus for this position will be computational modeling of learning and language. The ideal candidate will have previous experience in computational modeling, natural language processing, and programming in Python. Interested candidates should email Dr. Willits.
Undergraduate Research Assistants We are looking for motivated undergraduate students to assist with data collection and analysis in our lab.</description>
    </item>
    
    <item>
      <title>Members</title>
      <link>/members/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/members/</guid>
      <description>Principal Investigators Dr. Jessica L. Montag Email: jmontag@illinois.edu
Website: http://languagestats.com/jessicamontag/
Jessica Montag studies how language abilities emerge with experience across the lifespan, including language comprehension, production, and reading. Dr. Montag uses varied methods and measures, including comprehension and production experiments in adults and children, compilation and analyses of text and speech corpora, and advanced quantitative methods.
Dr. Jon A. Willits Email: jwillits@illinois.edu
Website: http://languagestats.com/jonwillits/
Jon Willits studies language and learning in infants, children, adults, and machines.</description>
    </item>
    
    <item>
      <title>Participate</title>
      <link>/participate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/participate/</guid>
      <description>Volunteers Needed for Our Research! If you have a child between about 6 months and 6 years old, we&amp;rsquo;d be thrilled if you and your child would participate in our studies of learning and language. Children are learning vast amounts of information from the earliest moments. We study how children learn language and other kinds of knowledge, including what words mean, how to talk and understand language, and how to read.</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/publications/</guid>
      <description>Peer-reviewed Papers Mao, S., Huebner, P., &amp;amp; Willits, J. A. (2023). Spatial vs. graphical representation of Distributional semantic knowledge. Psychological Review, in press. pdf
Flores, A. Z., Montag, J. L., &amp;amp; Willits, J. A. (2023). Using known words to learn more words: A distributional model of child vocabulary acquisition. Journal of Memory and Language, 132, 104446. pdf
Huebner, P., Willits, J. (2021). Scaffolded input promotes atomic organization in the recurrent neural network language model.</description>
    </item>
    
    <item>
      <title>Tools&#43;Data</title>
      <link>/tools&#43;data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/tools&#43;data/</guid>
      <description>Open-Source Python Packages Preppy This repository contains convenienve functions for preparing text data for training NLP models. It is possible to prepare batches of data that preserve the order of documents in the corpus.
Website: https://github.com/phueb/Preppy
Visualized This repository contains convenience functions for common visualizations, built on top of matplotlib.
Website: https://github.com/UIUCLearningLanguageLab/Visualized
Natural Language Data AO-CHILDES To create a custom text corpus of American-English child-directed language, use the Python package AOCHILDES.</description>
    </item>
    
    <item>
      <title>Topics</title>
      <link>/topics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/topics/</guid>
      <description>A brief overview of the topics we study in our lab.
Language Production Testing theories regarding how the structure of message-level representations build speech. Language Acquisition Studying how children acquire language via quantitative and qualitative analyses of language exposure. Computational Modeling Building computational models of semantic development and memory. Schizophrenia Measuring disorganized speech and semantic cohesion in speech of schizophrenic patients.
Developing automated language evaluation to help researchers better understand speech pathology.</description>
    </item>
    
  </channel>
</rss>
